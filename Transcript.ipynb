{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark - Key Features\n",
    "\n",
    "- Easy to use\n",
    "  - Comes with an expressive API - 80+ operators. \n",
    "  - Different levels of abstraction - RDDs, DataFrames, SparkSQL etc.\n",
    "  - API in - Java, Scala, Python, R\n",
    "  - Cluster Managers - Yarn, Mezos, Spark\n",
    "- Fast\n",
    "  - In-memory cluster computing capabilities\n",
    "  - Application can cache the data in memory - subsequent operations are performed on the cached data.\n",
    "  - Advanced job execution engine \n",
    "    - job converted to DAG of stages.\n",
    "    - job can contain several stages\n",
    "    - stage execution - optimized to minimize data movement\n",
    "- General Purpose\n",
    "  - batch processing\n",
    "  - interactive analysis\n",
    "  - stream processing\n",
    "  - machine learning\n",
    "  - graph computing\n",
    "- Scallable\n",
    "  - just add new nodes to the cluster\n",
    "- Fault tolerant\n",
    "  - 100+ node cluster high chance of failure\n",
    "  - Automatic node failure handling\n",
    "  - Failed tasks are automatically re-executed on different node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Architecture\n",
    "\n",
    "Terms:\n",
    "- Shuffle\n",
    "  - Redistributes data between the nodes.\n",
    "  - Expensive\n",
    "  - Not random - groups data on criteria into bucket\n",
    "  - each bucket - partition\n",
    "- Job\n",
    "  - An execution of a data processing algorithm.\n",
    "- Stage\n",
    "  - A collection of tasks in shuffle boundaries\n",
    "  - Tasks without shuffle - stage\n",
    "  - Tasks that require shuffle, start a new stage\n",
    "  - Stage depend on each other.\n",
    "  \n",
    "\n",
    "Five key entities:\n",
    "- Driver Program\n",
    "  - an application, uses Spark library - provides code, executed on worker nodes.\n",
    "  - can launch one or more jobs on the cluster.\n",
    "- Cluster Manager\n",
    "  - manages resources accross the cluster\n",
    "  - used to acquire resources for executing a job\n",
    "- Workers\n",
    "  - provide CPU, memory, storage resources to the application\n",
    "- Executors\n",
    "  - JVM processes created by Spark on each worker for an application\n",
    "  - lifetime as an application\n",
    "  - executes app code concurrently in multiple threads\n",
    "  - can cache data in memory or on disk\n",
    "- Tasks\n",
    "  - smallest unit of work sent to an executor.\n",
    "  - executed by a thread in an executor\n",
    "  - returns result to driver or provides data partition for shuffle\n",
    "  - task created per data pertition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Libraries\n",
    "- Spark Core\n",
    "- SparkSQL - SQL-like queries, DataFrame API\n",
    "  - Hive, Avro, Parquet, JSON, CSV, etc.\n",
    "  - Can combine data from different sources\n",
    "- SparkStreaming\n",
    "  - Ingest data from various sources at high velocity\n",
    "- Spark MLlib - Machine learning\n",
    "- GraphX - API for graph computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Core API\n",
    "\n",
    "- SparkContext\n",
    "  - Main entry point into the spark library\n",
    "  - Created by the application, using configuraiton.\n",
    "  - Single SparkContext per application\n",
    "- Resilient Distributed Dataset (RDD)\n",
    "  - Core data abstraction\n",
    "  - Immutable\n",
    "  - Parititioned\n",
    "  - Fault tolerant\n",
    "      - automatic failure handling\n",
    "      - Spark reconstructs lost partitions on another node.\n",
    "      - Spark stores lineage information for RDD - can reconstruct each part or entire RDD\n",
    "  - Abstract Programming Interface - independent from the underlying data provider - Hbase, Cassandra, JSON, Hadoop etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Programming Model\n",
    "- Create RDD\n",
    "  - textFile()\n",
    "  - parallelize()\n",
    "  - wholeTextFiles()\n",
    "      - key is the file path\n",
    "      - value is the file content\n",
    "  - sequenceFile() \n",
    "      - hadoop binary key-value file\n",
    "- Functional Programming\n",
    "- Create RDD\n",
    "- Transformations \n",
    "  - applied to an RDD, creates another RDD\n",
    "- Actions\n",
    "  - return value to the driver program\n",
    "- Save RDD\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 1\n",
    "Environment:\n",
    "- Cloudera CDH 5.7.0 Quick Start VM, running on Virtual Box 5.0 in Windows\n",
    "- Python 3\n",
    "- Jupyter Notebook (Could also use Apache Zeppelin â€“ still in baby stage)\n",
    "- Apache Spark 2.0.1 for Hadoop 2.7 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "# Local dataset directory\n",
    "datasets_local = 'data'\n",
    "\n",
    "# Hadoop HDFS url\n",
    "datasets_hdfs = 'hdfs://localhost:8020/user/cloudera/datasets'\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "datasets = datasets_local\n",
    "ml_used = 'ml-latest'    # Which MovieLens dataset to use\n",
    "ml_datasets = datasets + '/' + ml_used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Wordcount\n",
    "\n",
    "Let's start with a classical wordcount example. We will use direct list of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create lines RDD using direct list of lines and the parallelize() method from SparkContext\n",
    "rdd_lines = sc.parallelize(['Down, down, down. There was nothing else to do,','so Alice soon began talking again.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Down, down, down. There was nothing else to do,',\n",
       " 'so Alice soon began talking again.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine our lines RDD\n",
    "rdd_lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We see that some cleaning work is to be done.\n",
    "# Let's replace the dots, commas and dashes with spaces.\n",
    "rdd_clean_liness = rdd_lines.map(lambda txt: txt.replace(',', ' ').replace('.', ' ').replace('-', ' ').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down  down  down  there was nothing else to do ',\n",
       " 'so alice soon began talking again ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And check the result\n",
    "rdd_clean_liness.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can split the lines into individual words, using flatMap.\n",
    "# flatMap can produce multiple records from a single input record.\n",
    "rdd_words = rdd_clean_liness.flatMap(lambda txt: txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down',\n",
       " 'down',\n",
       " 'down',\n",
       " 'there',\n",
       " 'was',\n",
       " 'nothing',\n",
       " 'else',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " 'alice',\n",
       " 'soon',\n",
       " 'began',\n",
       " 'talking',\n",
       " 'again']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How the words RDD looks?\n",
    "rdd_words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'again': 1,\n",
       "             'alice': 1,\n",
       "             'began': 1,\n",
       "             'do': 1,\n",
       "             'down': 3,\n",
       "             'else': 1,\n",
       "             'nothing': 1,\n",
       "             'so': 1,\n",
       "             'soon': 1,\n",
       "             'talking': 1,\n",
       "             'there': 1,\n",
       "             'to': 1,\n",
       "             'was': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And finally our counts - as a dictionary\n",
    "word_counts = rdd_words.countByValue()\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('again', 1), ('so', 1), ('was', 1), ('nothing', 1), ('to', 1), ('else', 1), ('soon', 1), ('do', 1), ('began', 1), ('talking', 1), ('alice', 1), ('there', 1), ('down', 3)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to sort the counts, we can use the items() method to get a list of (word, count) tuples:\n",
    "word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('down', 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And sort them\n",
    "sorted_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# And maybe take only the top word.\n",
    "sorted_counts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. Wordcount from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã¯Â»Â¿Project GutenbergÃ¢â‚¬â„¢s AliceÃ¢â‚¬â„¢s Adventures in Wonderland, by Lewis Carroll\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: AliceÃ¢â‚¬â„¢s Adventures in Wonderland\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! head data/book-allice-wonderland.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1818), ('and', 940), ('to', 809), ('a', 690), ('of', 631)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_lines = sc.textFile('data/book-allice-wonderland.txt')\n",
    "#rdd_words = rdd_lines.map(lambda txt: txt.replace(',', ' ').replace('.', ' ').replace('-', ' ').lower()) \\\n",
    "rdd_words = rdd_lines.map(lambda txt: re.sub('\\W', ' ', txt).lower()) \\\n",
    "         .flatMap(lambda txt: txt.split())\n",
    "word_counts = rdd_words.countByValue()\n",
    "\n",
    "sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file out already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_counts = sc.parallelize(word_counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm -rf out/counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_counts.saveAsTextFile('out/counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 93\n",
      "drwxrwxrwx   1 baobab          baobab                0 Nov 27 18:20 .\n",
      "drwxrwxrwx   1 baobab          baobab                0 Nov 27 18:20 ..\n",
      "-rwxrwxrwa   1 baobab          baobab                8 Nov 27 18:20 ._SUCCESS.crc\n",
      "-rwxrwxrwa   1 baobab          baobab               96 Nov 27 18:20 .part-00000.crc\n",
      "-rwxrwxrwa   1 baobab          baobab               96 Nov 27 18:20 .part-00001.crc\n",
      "-rwxrwxrwa   1 baobab          baobab               96 Nov 27 18:20 .part-00002.crc\n",
      "-rwxrwxrwa   1 baobab          baobab               96 Nov 27 18:20 .part-00003.crc\n",
      "-rwxrwxrwa   1 baobab          baobab                0 Nov 27 18:20 _SUCCESS\n",
      "-rwxrwxrwa   1 baobab          baobab            11030 Nov 27 18:20 part-00000\n",
      "-rwxrwxrwa   1 baobab          baobab            10996 Nov 27 18:20 part-00001\n",
      "-rwxrwxrwa   1 baobab          baobab            10918 Nov 27 18:20 part-00002\n",
      "-rwxrwxrwa   1 baobab          baobab            10980 Nov 27 18:20 part-00003\n"
     ]
    }
   ],
   "source": [
    "!ls -al out/counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame API\n",
    "\n",
    "Sprark DataFrame API builds on the core RDD API, by providing rich and familiar set of operations.\n",
    "The DataFrame API and the SparkSQL API are available through a SparkSession object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 - DataFrame and JSON\n",
    "\n",
    "For this example we are going to zip code data. It is in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can create a DataFrame by reading from a data source.\n",
    "df_zips = spark.read.json(datasets + '/zips.json')\n",
    "type(df_zips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+-----+-----+\n",
      "|  _id|       city|                 loc|  pop|state|\n",
      "+-----+-----------+--------------------+-----+-----+\n",
      "|01001|     AGAWAM|[-72.622739, 42.0...|15338|   MA|\n",
      "|01002|    CUSHMAN|[-72.51565, 42.37...|36963|   MA|\n",
      "|01005|      BARRE|[-72.108354, 42.4...| 4546|   MA|\n",
      "|01007|BELCHERTOWN|[-72.410953, 42.2...|10579|   MA|\n",
      "|01008|  BLANDFORD|[-72.936114, 42.1...| 1240|   MA|\n",
      "+-----+-----------+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 rows to the console\n",
    "df_zips.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               pop|\n",
      "+-------+------------------+\n",
      "|  count|             29353|\n",
      "|   mean| 8462.794262937348|\n",
      "| stddev|12329.680305853608|\n",
      "|    min|                 0|\n",
      "|    max|            112047|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can get some summary statistics about numeric data, using describe()\n",
    "df_zips.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|state|\n",
      "+-----+\n",
      "|   SC|\n",
      "|   AZ|\n",
      "|   LA|\n",
      "|   MN|\n",
      "|   NJ|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zips[['state']].distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| sum(pop)|\n",
      "+---------+\n",
      "|248408400|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "# How many people make the entire population in US according to our data?\n",
    "df_zips.agg(sf.sum('pop')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state='CA', pop=29754890),\n",
       " Row(state='NY', pop=17990402),\n",
       " Row(state='TX', pop=16984601),\n",
       " Row(state='FL', pop=12686644),\n",
       " Row(state='PA', pop=11881643)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which are the 5 most populated states?\n",
    "df_states = df_zips.groupBy('state').agg(sf.sum('pop').alias('pop')).orderBy(sf.desc('pop'))\n",
    "df_states.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Let's do some charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA</td>\n",
       "      <td>29754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NY</td>\n",
       "      <td>17990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TX</td>\n",
       "      <td>16984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FL</td>\n",
       "      <td>12686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PA</td>\n",
       "      <td>11881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state    pop\n",
       "0    CA  29754\n",
       "1    NY  17990\n",
       "2    TX  16984\n",
       "3    FL  12686\n",
       "4    PA  11881"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_states = df_states.toPandas()\n",
    "pd_states['pop'] = (pd_states['pop']/1000).astype('int')\n",
    "pd_states.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsyIOGGZmVsQBw8zMijhgmJlZkf8PaE0r/08ue3QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c447048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.title('State population distribution')\n",
    "plt.ylabel('Number of States')\n",
    "plt.hist(pd_states['pop'], bins=20)\n",
    "plt.subplot(212)\n",
    "plt.boxplot(pd_states['pop'], vert=False)\n",
    "plt.xlabel('population, thousandths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state|   pop|\n",
      "+-----+------+\n",
      "|   WY|453528|\n",
      "|   AK|544698|\n",
      "|   VT|562758|\n",
      "|   DC|606900|\n",
      "|   ND|638272|\n",
      "+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_states.orderBy('pop', ascending = True).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens Dataset\n",
    "\n",
    "In our examples we are using a popular MovieLens dataset.\n",
    "\n",
    "### Download MovieLens dataset\n",
    "Because the filesize is big (>150 MB compressed) we are going to download the files from the GorupLens web site.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "import os\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset exists.\n"
     ]
    }
   ],
   "source": [
    "def install_ml_dataset(dataset_name):\n",
    "    # Download\n",
    "    zip_filename = datasets_local + '/' + dataset_name + '.zip'\n",
    "    dataset_dir = datasets_local  + '/' + dataset_name\n",
    "    if os.path.exists(dataset_dir):\n",
    "        print('Dataset exists.')\n",
    "        return\n",
    "    print('Dataset directory not found. Will try to install it.')\n",
    "    if os.path.exists(zip_filename):\n",
    "        print('File %s found. Going to use it.' % zip_filename)\n",
    "    else:\n",
    "        url = 'http://files.grouplens.org/datasets/movielens/' + dataset_name + '.zip'\n",
    "        print('File %s doesn exists. Going to download from %s' % (zip_filename, url) )\n",
    "        print(\"Might take a while. Please be patient...\")\n",
    "        with urlopen(url) as infile:\n",
    "            data = infile.read()\n",
    "        with open(zip_filename, 'wb') as outfile:\n",
    "            outfile.write(data)\n",
    "        print(\"Download complete. Saved %d bytes.\" % os.path.getsize(zip_filename))\n",
    "    \n",
    "    unzip_command = 'unzip %s -d %s' % (zip_filename, datasets_local)\n",
    "    print('Extract using command: \\n%s' % (unzip_command))\n",
    "    if os.system(unzip_command):\n",
    "        raise Exception('Unable to extract file. Please try to execute manually.')\n",
    "    else:\n",
    "        print(\"Dataset installed successfully.\")\n",
    "    \n",
    "    \n",
    "install_ml_dataset(ml_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the MovieLens dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.txt\n",
      "genome-scores.csv\n",
      "genome-tags.csv\n",
      "links.csv\n",
      "movies.csv\n",
      "parquet\n",
      "ratings.csv\n",
      "tags.csv\n"
     ]
    }
   ],
   "source": [
    "# What files we have in our MovieLens dataset\n",
    "! ls $datasets_local/$ml_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's collect some summary info into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We will measure the cell execution time using the IPython's time magic\n",
    "data_info = []\n",
    "\n",
    "\n",
    "# For each CSV file in our local MovieLens dataset\n",
    "#   - get the data name\n",
    "#   - get the filesize\n",
    "#   - load the CSV and get the number of records\n",
    "for csv_filename in glob.glob(datasets_local + '/' + ml_used + '/*.csv'):\n",
    "    name = os.path.basename(csv_filename)[:-4]\n",
    "    file_size = os.path.getsize(csv_filename)\n",
    "    ds = spark.read.csv(csv_filename, header=True, inferSchema=True)\n",
    "    rec_count = ds.count()\n",
    "    data_info.append( (csv_filename, name, file_size, rec_count) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create pandas DataFrame\n",
    "df_ml = pd.DataFrame(data_info, columns=['filename', 'name','file_size', 'record_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We could use the humanize (https://pypi.python.org/pypi/humanize) Python package, but \n",
    "# is not a big deal to create a custom function.\n",
    "def sizeof_fmt(num, fmt=\"%0.1f %s%s\", multiplier=1024.0, units=None, suffix='B'):\n",
    "    # Handle optional arguments\n",
    "    multiplier = float(multiplier)\n",
    "    if not units:\n",
    "        units = ['','K','M','G','T','P','E','Z','Y']\n",
    "        \n",
    "    for unit in units[:-1]:\n",
    "        if abs(num) < multiplier:\n",
    "            return fmt % (num, unit, suffix)\n",
    "        num /= multiplier\n",
    "    return fmt % (num, units[-1], suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.0 B\n",
      "1.2 KB\n",
      "1.2 MB\n",
      "1.1 GB\n",
      "1.1 TB\n",
      "1.1 PB\n",
      "1.1 EB\n",
      "1.0 ZB\n",
      "1.0 YB\n",
      "1021.211 YB\n",
      "1021 YB\n"
     ]
    }
   ],
   "source": [
    "# Let's test our fomratting function\n",
    "print(sizeof_fmt(123))\n",
    "print(sizeof_fmt(1234))\n",
    "print(sizeof_fmt(1234567))\n",
    "print(sizeof_fmt(1234567890))\n",
    "print(sizeof_fmt(1234567890123))\n",
    "print(sizeof_fmt(1234567890123456))\n",
    "print(sizeof_fmt(1234567890123456789))\n",
    "print(sizeof_fmt(1234567890123456789012))\n",
    "print(sizeof_fmt(1234567890123456789012345))\n",
    "print(sizeof_fmt(1234567890123456789012345678, fmt='%0.3f %s%s'))\n",
    "print(sizeof_fmt(1234567890123456789012345678, fmt='%0d %s%s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>file_size</th>\n",
       "      <th>record_count</th>\n",
       "      <th>File Size</th>\n",
       "      <th>Num Records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>genome-scores</td>\n",
       "      <td>333365341</td>\n",
       "      <td>12040272</td>\n",
       "      <td>317.9 MB</td>\n",
       "      <td>11.5 M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>genome-tags</td>\n",
       "      <td>18103</td>\n",
       "      <td>1128</td>\n",
       "      <td>17.7 KB</td>\n",
       "      <td>1.1 K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>links</td>\n",
       "      <td>859311</td>\n",
       "      <td>40110</td>\n",
       "      <td>839.2 KB</td>\n",
       "      <td>39.2 K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movies</td>\n",
       "      <td>2007982</td>\n",
       "      <td>40110</td>\n",
       "      <td>1.9 MB</td>\n",
       "      <td>39.2 K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ratings</td>\n",
       "      <td>663420664</td>\n",
       "      <td>24404096</td>\n",
       "      <td>632.7 MB</td>\n",
       "      <td>23.3 M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tags</td>\n",
       "      <td>24032991</td>\n",
       "      <td>668953</td>\n",
       "      <td>22.9 MB</td>\n",
       "      <td>653.3 K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  file_size  record_count File Size Num Records\n",
       "0  genome-scores  333365341      12040272  317.9 MB      11.5 M\n",
       "1    genome-tags      18103          1128   17.7 KB       1.1 K\n",
       "2          links     859311         40110  839.2 KB      39.2 K\n",
       "3         movies    2007982         40110    1.9 MB      39.2 K\n",
       "4        ratings  663420664      24404096  632.7 MB      23.3 M\n",
       "5           tags   24032991        668953   22.9 MB     653.3 K"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A little humanization\n",
    "df_ml['File Size'] = df_ml['file_size'].map(sizeof_fmt)\n",
    "df_ml['Num Records'] = df_ml['record_count'].map(lambda x: sizeof_fmt(x, suffix=''))\n",
    "df_ml[df_ml.columns[1:]]   # Skip the filename column. Could be long and is not interesting here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4. Parquet files\n",
    "\n",
    "Spark supports various input formats out of the box. One of the popular file formats is Parquet. Parquet is a columnar file format with RLE. Let's compare the performance of csv and parquet files.\n",
    "\n",
    "We are going to use the popular MovieLens dataset - collected information about movie ratings from visitors.\n",
    "\n",
    "The Movielens dataset is in two sizes: small and full. The small dataset presents a 10% movie sample over the full dataset We are going to use the **full size** just to demonstrate the power and speed of Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Parquet files from CSV\n",
    "\n",
    "We are going to convert our MovieLens dataset files into Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for file_index in df_ml.index:\n",
    "    file_info = df_ml.iloc[file_index]\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = spark.read.csv(file_info['filename'], header=True, inferSchema=True)\n",
    "    \n",
    "    # Save Parquet format\n",
    "    df.write.parquet(ml_datasets + '/parquet/' + file_info['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18.81MB data/ml-latest/parquet/genome-scores\n",
      "   17.00KB data/ml-latest/parquet/genome-tags\n",
      "  476.00KB data/ml-latest/parquet/links\n",
      "    1.02MB data/ml-latest/parquet/movies\n",
      "  144.10MB data/ml-latest/parquet/ratings\n",
      "    6.99MB data/ml-latest/parquet/tags\n",
      "  171.40MB data/ml-latest/parquet\n"
     ]
    }
   ],
   "source": [
    "# What is the total disk usage for Parqute files?\n",
    "! du -h $ml_datasets/parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'976.3 MB'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the total usage for CSV files?\n",
    "sizeof_fmt(sum(df_ml['file_size']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Parquet vs CSV performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are going to use the embedded timeit IPython magic.\n",
    "# Let's see its documentation.\n",
    "\n",
    "%%timeit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 2: 26.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 2\n",
    "spark.read.csv(ml_datasets + '/ratings.csv', header=True, inferSchema=True).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 loops, best of 1: 422 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 2 -r 1\n",
    "spark.read.parquet(ml_datasets + '/parquet/ratings').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5 - Explore MovieLens with Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_rating = spark.read.parquet(ml_datasets + '/parquet/ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|    122|   2.0|945544824|\n",
      "|     1|    172|   1.0|945544871|\n",
      "|     1|   1221|   5.0|945544788|\n",
      "|     1|   1441|   4.0|945544871|\n",
      "|     1|   1609|   3.0|945544824|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rating.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24404096"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many ratings do we have?\n",
    "df_rating.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(userId,IntegerType,true),StructField(movieId,IntegerType,true),StructField(rating,DoubleType,true),StructField(timestamp,IntegerType,true)))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fields have type\n",
    "df_rating.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5011914"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to know how many movies have rating 3\n",
    "df_rating_3 = df_rating.filter('rating = 3')\n",
    "\n",
    "df_rating_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see some of the movies with rating 3\n",
    "\n",
    "df_movies = spark.read.parquet(ml_datasets + '/parquet/movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movies.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join movies and ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|               title|              genres|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|   1609|     1|   3.0| 945544824|187 (One Eight Se...|      Drama|Thriller|\n",
      "|   1961|     1|   3.0| 945544871|     Rain Man (1988)|               Drama|\n",
      "|   1597|     2|   3.0|1008942773|Conspiracy Theory...|Drama|Mystery|Rom...|\n",
      "|   1608|     2|   3.0|1008942733|Air Force One (1997)|     Action|Thriller|\n",
      "|   4963|     2|   3.0|1008942667|Ocean's Eleven (2...|      Crime|Thriller|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 938 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_rating_3.join(df_movies, on='movieId').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load source files into DataFrames\n",
    "df_movies = spark.read.parquet(ml_datasets + '/parquet/movies')\n",
    "df_ratings = spark.read.parquet(ml_datasets + '/parquet/ratings')\n",
    "df_tags = spark.read.parquet(ml_datasets + '/parquet/genome-tags')\n",
    "df_scores = spark.read.parquet(ml_datasets + '/parquet/genome-scores')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies.createOrReplaceTempView('movies')\n",
    "df_ratings.createOrReplaceTempView('ratings')\n",
    "df_tags.createOrReplaceTempView('gtags')\n",
    "df_scores.createOrReplaceTempView('gscores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movies.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------------------+\n",
      "|movieId|tagId|          relevance|\n",
      "+-------+-----+-------------------+\n",
      "|      1|    1|0.02400000000000002|\n",
      "|      1|    2|0.02400000000000002|\n",
      "|      1|    3|0.05475000000000002|\n",
      "|      1|    4|0.09200000000000003|\n",
      "|      1|    5|            0.14825|\n",
      "+-------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scores.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|tagId|         tag|\n",
      "+-----+------------+\n",
      "|    1|         007|\n",
      "|    2|007 (series)|\n",
      "|    3|18th century|\n",
      "|    4|       1920s|\n",
      "|    5|       1930s|\n",
      "+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tags.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|   4306|        Shrek (2001)|Adventure|Animati...|\n",
      "|   8360|      Shrek 2 (2004)|Adventure|Animati...|\n",
      "|  53121|Shrek the Third (...|Adventure|Animati...|\n",
      "|  64249|Shrek the Halls (...|Adventure|Animati...|\n",
      "|  78637|Shrek Forever Aft...|Adventure|Animati...|\n",
      "| 109965|Shrek the Musical...|Children|Comedy|F...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM movies WHERE title LIKE 'Shrek%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------------------+---------------+\n",
      "|movieId|tagId|           relevance|            tag|\n",
      "+-------+-----+--------------------+---------------+\n",
      "|   4306|    1| 0.03149999999999997|            007|\n",
      "|   4306|    2|0.028749999999999998|   007 (series)|\n",
      "|   4306|    3|0.035999999999999976|   18th century|\n",
      "|   4306|    4|0.058750000000000024|          1920s|\n",
      "|   4306|    5| 0.12674999999999997|          1930s|\n",
      "|   4306|    6| 0.09200000000000003|          1950s|\n",
      "|   4306|    7| 0.02400000000000002|          1960s|\n",
      "|   4306|    8|               0.182|          1970s|\n",
      "|   4306|    9|             0.09025|          1980s|\n",
      "|   4306|   10| 0.04149999999999998|   19th century|\n",
      "|   4306|   11|              0.2855|             3d|\n",
      "|   4306|   12|0.057499999999999996|           70mm|\n",
      "|   4306|   13| 0.06624999999999998|            80s|\n",
      "|   4306|   14|0.012749999999999984|           9/11|\n",
      "|   4306|   15|             0.03075|        aardman|\n",
      "|   4306|   16|              0.2375|aardman studios|\n",
      "|   4306|   17|0.010749999999999982|       abortion|\n",
      "|   4306|   18|             0.33375|         absurd|\n",
      "|   4306|   19| 0.45399999999999996|         action|\n",
      "|   4306|   20|              0.2405|  action packed|\n",
      "+-------+-----+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shrek_tags = spark.sql(\"SELECT gs.movieId, gs.tagId, gs.relevance, gt.tag FROM gscores gs JOIN gtags gt ON gs.tagId=gt.tagId WHERE movieId=4306\")\n",
    "shrek_tags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [movieId#545, tagId#546, relevance#547, tag#541]\n",
      "+- *BroadcastHashJoin [tagId#546], [tagId#540], Inner, BuildRight\n",
      "   :- *Project [movieId#545, tagId#546, relevance#547]\n",
      "   :  +- *Filter ((isnotnull(movieId#545) && (movieId#545 = 4306)) && isnotnull(tagId#546))\n",
      "   :     +- *BatchedScan parquet [movieId#545,tagId#546,relevance#547] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/genome-scores, PartitionFilters: [], PushedFilters: [IsNotNull(movieId), EqualTo(movieId,4306), IsNotNull(tagId)], ReadSchema: struct<movieId:int,tagId:int,relevance:double>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "      +- *Project [tagId#540, tag#541]\n",
      "         +- *Filter isnotnull(tagId#540)\n",
      "            +- *BatchedScan parquet [tagId#540,tag#541] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/genome-tags, PartitionFilters: [], PushedFilters: [IsNotNull(tagId)], ReadSchema: struct<tagId:int,tag:string>\n"
     ]
    }
   ],
   "source": [
    "shrek_tags.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shrek_tags.createOrReplaceTempView('shrek_tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_relevant = spark.sql(\"\"\"\n",
    "    SELECT gs.movieId,\n",
    "           m.title,\n",
    "           gs.tagId,\n",
    "           gs.relevance,\n",
    "           t.tag\n",
    "      FROM gscores gs\n",
    "      JOIN shrek_tags ss ON ss.tagId=gs.tagId AND ss.movieId=4306\n",
    "      JOIN movies m ON m.movieId=gs.movieId\n",
    "      JOIN gtags t ON t.tagId=gs.tagId\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----+--------------------+----+\n",
      "|movieId|               title|tagId|           relevance| tag|\n",
      "+-------+--------------------+-----+--------------------+----+\n",
      "|      1|    Toy Story (1995)|  148| 0.02200000000000002|boat|\n",
      "|      2|      Jumanji (1995)|  148| 0.02300000000000002|boat|\n",
      "|      3|Grumpier Old Men ...|  148| 0.09200000000000003|boat|\n",
      "|      4|Waiting to Exhale...|  148| 0.03149999999999997|boat|\n",
      "|      5|Father of the Bri...|  148|0.060250000000000026|boat|\n",
      "|      6|         Heat (1995)|  148| 0.02024999999999999|boat|\n",
      "|      7|      Sabrina (1995)|  148|0.056999999999999995|boat|\n",
      "|      8| Tom and Huck (1995)|  148| 0.04325000000000001|boat|\n",
      "|      9| Sudden Death (1995)|  148|0.020000000000000018|boat|\n",
      "|     10|    GoldenEye (1995)|  148|             0.09425|boat|\n",
      "+-------+--------------------+-----+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_relevant.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_relevant_improve = spark.sql(\"\"\"\n",
    "    SELECT gs.movieId,\n",
    "           m.title,\n",
    "           gs.tagId,\n",
    "           gs.relevance,\n",
    "           t.tag,\n",
    "           ss.relevance AS shrek_relevance\n",
    "      FROM gscores gs\n",
    "      JOIN shrek_tags ss ON ss.tagId=gs.tagId AND ss.movieId=4306 AND ss.relevance > 0.8\n",
    "      JOIN movies m ON m.movieId=gs.movieId\n",
    "      JOIN gtags t ON t.tagId=gs.tagId\n",
    "     WHERE gs.relevance > 0.8\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----+------------------+----+---------------+\n",
      "|movieId|               title|tagId|         relevance| tag|shrek_relevance|\n",
      "+-------+--------------------+-----+------------------+----+---------------+\n",
      "|      1|    Toy Story (1995)|  588|           0.97875|kids|         0.9595|\n",
      "|      2|      Jumanji (1995)|  588|             0.958|kids|         0.9595|\n",
      "|     54|Big Green, The (1...|  588|           0.93025|kids|         0.9595|\n",
      "|     56|Kids of the Round...|  588|0.9652499999999999|kids|         0.9595|\n",
      "|     87|Dunston Checks In...|  588|            0.8225|kids|         0.9595|\n",
      "|    146|Amazing Panda Adv...|  588|0.8614999999999999|kids|         0.9595|\n",
      "|    158|       Casper (1995)|  588|0.9312499999999999|kids|         0.9595|\n",
      "|    239|Goofy Movie, A (1...|  588|0.8402499999999999|kids|         0.9595|\n",
      "|    250|Heavyweights (Hea...|  588|0.8799999999999999|kids|         0.9595|\n",
      "|    258|Kid in King Arthu...|  588|           0.84425|kids|         0.9595|\n",
      "+-------+--------------------+-----+------------------+----+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_relevant_improve.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_relevant_improve.cache().createOrReplaceTempView('relevant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|               rel|movieId|               title|\n",
      "+------------------+-------+--------------------+\n",
      "|            0.9935|   5419|   Scooby-Doo (2002)|\n",
      "|           0.98875|   2147|Clan of the Cave ...|\n",
      "|0.9882500000000001|   3435|Double Indemnity ...|\n",
      "|0.9877499999999999|  91500|The Hunger Games ...|\n",
      "|0.9877499999999999|   1178|Paths of Glory (1...|\n",
      "|             0.986| 118997|Into the Woods (2...|\n",
      "|           0.98525|  63239|   Cinderella (1997)|\n",
      "|0.9850000000000001|   1212|Third Man, The (1...|\n",
      "|0.9850000000000001|   4298|Rififi (Du rififi...|\n",
      "|           0.98475|  59615|Indiana Jones and...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT AVG(relevance) AS rel, movieId, title FROM relevant GROUP BY movieId, title ORDER BY rel DESC').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|               rel|movieId|               title|\n",
      "+------------------+-------+--------------------+\n",
      "|            0.9955|   2720|Inspector Gadget ...|\n",
      "|0.9944999999999999|    905|It Happened One N...|\n",
      "|             0.994|   2726| Killing, The (1956)|\n",
      "|0.9937499999999999|  63540|Beverly Hills Chi...|\n",
      "|            0.9935|   5419|   Scooby-Doo (2002)|\n",
      "|           0.99325|   1945|On the Waterfront...|\n",
      "|          0.992875|  36401|Brothers Grimm, T...|\n",
      "|           0.99125|   3468| Hustler, The (1961)|\n",
      "|           0.99075|    923| Citizen Kane (1941)|\n",
      "|           0.99075|   1193|One Flew Over the...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Wall time: 6.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_suggested = spark.sql('SELECT AVG(relevance) AS rel, movieId, title FROM relevant WHERE shrek_relevance > 0.9 GROUP BY movieId, title ORDER BY rel DESC')\n",
    "df_suggested.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Sort [rel#719 DESC], true, 0\n",
      "+- Exchange rangepartitioning(rel#719 DESC, 200)\n",
      "   +- *HashAggregate(keys=[movieId#545, title#525], functions=[avg(relevance#547)])\n",
      "      +- Exchange hashpartitioning(movieId#545, title#525, 200)\n",
      "         +- *HashAggregate(keys=[movieId#545, title#525], functions=[partial_avg(relevance#547)])\n",
      "            +- *Project [movieId#545, title#525, relevance#547]\n",
      "               +- *Filter (isnotnull(shrek_relevance#614) && (shrek_relevance#614 > 0.9))\n",
      "                  +- InMemoryTableScan [movieId#545, title#525, relevance#547, shrek_relevance#614], [isnotnull(shrek_relevance#614), (shrek_relevance#614 > 0.9)]\n",
      "                     :  +- InMemoryRelation [movieId#545, title#525, tagId#546, relevance#547, tag#630, shrek_relevance#614], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     :     :  +- *Project [movieId#545, title#525, tagId#546, relevance#547, tag#630, relevance#628 AS shrek_relevance#614]\n",
      "                     :     :     +- *BroadcastHashJoin [tagId#546], [tagId#629], Inner, BuildRight\n",
      "                     :     :        :- *Project [movieId#545, tagId#546, relevance#547, relevance#628, title#525]\n",
      "                     :     :        :  +- *BroadcastHashJoin [movieId#545], [movieId#524], Inner, BuildRight\n",
      "                     :     :        :     :- *Project [movieId#545, tagId#546, relevance#547, relevance#628]\n",
      "                     :     :        :     :  +- *SortMergeJoin [tagId#546], [tagId#627], Inner\n",
      "                     :     :        :     :     :- *Sort [tagId#546 ASC], false, 0\n",
      "                     :     :        :     :     :  +- Exchange hashpartitioning(tagId#546, 200)\n",
      "                     :     :        :     :     :     +- *Project [movieId#545, tagId#546, relevance#547]\n",
      "                     :     :        :     :     :        +- *Filter (((isnotnull(relevance#547) && (relevance#547 > 0.8)) && isnotnull(tagId#546)) && isnotnull(movieId#545))\n",
      "                     :     :        :     :     :           +- *BatchedScan parquet [movieId#545,tagId#546,relevance#547] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/genome-scores, PartitionFilters: [], PushedFilters: [IsNotNull(relevance), GreaterThan(relevance,0.8), IsNotNull(tagId), IsNotNull(movieId)], ReadSchema: struct<movieId:int,tagId:int,relevance:double>\n",
      "                     :     :        :     :     +- *Sort [tagId#627 ASC], false, 0\n",
      "                     :     :        :     :        +- Exchange hashpartitioning(tagId#627, 200)\n",
      "                     :     :        :     :           +- *Project [tagId#627, relevance#628]\n",
      "                     :     :        :     :              +- *BroadcastHashJoin [tagId#627], [tagId#540], Inner, BuildRight\n",
      "                     :     :        :     :                 :- *Project [tagId#627, relevance#628]\n",
      "                     :     :        :     :                 :  +- *Filter ((((isnotnull(movieId#626) && (movieId#626 = 4306)) && isnotnull(relevance#628)) && (relevance#628 > 0.8)) && isnotnull(tagId#627))\n",
      "                     :     :        :     :                 :     +- *BatchedScan parquet [movieId#626,tagId#627,relevance#628] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/genome-scores, PartitionFilters: [], PushedFilters: [IsNotNull(movieId), EqualTo(movieId,4306), IsNotNull(relevance), GreaterThan(relevance,0.8), IsN..., ReadSchema: struct<movieId:int,tagId:int,relevance:double>\n",
      "                     :     :        :     :                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "                     :     :        :     :                    +- *Project [tagId#540]\n",
      "                     :     :        :     :                       +- *Filter isnotnull(tagId#540)\n",
      "                     :     :        :     :                          +- *BatchedScan parquet [tagId#540] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/genome-tags, PartitionFilters: [], PushedFilters: [IsNotNull(tagId)], ReadSchema: struct<tagId:int>\n",
      "                     :     :        :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "                     :     :        :        +- *Project [movieId#524, title#525]\n",
      "                     :     :        :           +- *Filter isnotnull(movieId#524)\n",
      "                     :     :        :              +- *BatchedScan parquet [movieId#524,title#525] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/movies, PartitionFilters: [], PushedFilters: [IsNotNull(movieId)], ReadSchema: struct<movieId:int,title:string>\n",
      "                     :     :        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "                     :     :           +- *Project [tagId#629, tag#630]\n",
      "                     :     :              +- *Filter isnotnull(tagId#629)\n",
      "                     :     :                 +- *BatchedScan parquet [tagId#629,tag#630] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/genome-tags, PartitionFilters: [], PushedFilters: [IsNotNull(tagId)], ReadSchema: struct<tagId:int,tag:string>\n"
     ]
    }
   ],
   "source": [
    "df_suggested.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/D:/Sandbox/spark-intro/git/spark-intro/spark-warehouse')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='like_shrek', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='gscores', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='gtags', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='movies', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='ratings', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='relevant', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='shrek_tags', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can materialize our suggestions to a physical table\n",
    "spark.sql('DROP TABLE IF EXISTS default.like_shrek')\n",
    "spark.sql('CREATE TABLE default.like_shrek AS SELECT AVG(relevance) AS rel, movieId, title FROM relevant WHERE shrek_relevance > 0.9 GROUP BY movieId, title ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='like_shrek', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='gscores', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='gtags', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='movies', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='ratings', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='relevant', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='shrek_tags', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can see our table is not temporary\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|               rel|movieid|               title|\n",
      "+------------------+-------+--------------------+\n",
      "|            0.9955|   2720|Inspector Gadget ...|\n",
      "|0.9944999999999999|    905|It Happened One N...|\n",
      "|             0.994|   2726| Killing, The (1956)|\n",
      "|0.9937499999999999|  63540|Beverly Hills Chi...|\n",
      "|            0.9935|   5419|   Scooby-Doo (2002)|\n",
      "|           0.99325|   1945|On the Waterfront...|\n",
      "|          0.992875|  36401|Brothers Grimm, T...|\n",
      "|           0.99125|   3468| Hustler, The (1961)|\n",
      "|           0.99075|    923| Citizen Kane (1941)|\n",
      "|           0.99075|   1193|One Flew Over the...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_like_shrek = spark.sql('SELECT * FROM like_shrek ORDER BY rel DESC')\n",
    "df_like_shrek.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Sort [rel#898 DESC], true, 0\n",
      "+- Exchange rangepartitioning(rel#898 DESC, 200)\n",
      "   +- HiveTableScan [rel#898, movieid#899, title#900], MetastoreRelation default, like_shrek\n"
     ]
    }
   ],
   "source": [
    "df_like_shrek.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [movieId#545, title#525, tagId#546, relevance#547, tag#630, shrek_relevance#614]\n",
      ":  +- InMemoryRelation [movieId#545, title#525, tagId#546, relevance#547, tag#630, shrek_relevance#614], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      ":     :  +- *Project [movieId#545, title#525, tagId#546, relevance#547, tag#630, relevance#628 AS shrek_relevance#614]\n",
      ":     :     +- *BroadcastHashJoin [tagId#546], [tagId#629], Inner, BuildRight\n",
      ":     :        :- *Project [movieId#545, tagId#546, relevance#547, relevance#628, title#525]\n",
      ":     :        :  +- *BroadcastHashJoin [movieId#545], [movieId#524], Inner, BuildRight\n",
      ":     :        :     :- *Project [movieId#545, tagId#546, relevance#547, relevance#628]\n",
      ":     :        :     :  +- *SortMergeJoin [tagId#546], [tagId#627], Inner\n",
      ":     :        :     :     :- *Sort [tagId#546 ASC], false, 0\n",
      ":     :        :     :     :  +- Exchange hashpartitioning(tagId#546, 200)\n",
      ":     :        :     :     :     +- *Project [movieId#545, tagId#546, relevance#547]\n",
      ":     :        :     :     :        +- *Filter (((isnotnull(relevance#547) && (relevance#547 > 0.8)) && isnotnull(tagId#546)) && isnotnull(movieId#545))\n",
      ":     :        :     :     :           +- *BatchedScan parquet [movieId#545,tagId#546,relevance#547] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/genome-scores, PartitionFilters: [], PushedFilters: [IsNotNull(relevance), GreaterThan(relevance,0.8), IsNotNull(tagId), IsNotNull(movieId)], ReadSchema: struct<movieId:int,tagId:int,relevance:double>\n",
      ":     :        :     :     +- *Sort [tagId#627 ASC], false, 0\n",
      ":     :        :     :        +- Exchange hashpartitioning(tagId#627, 200)\n",
      ":     :        :     :           +- *Project [tagId#627, relevance#628]\n",
      ":     :        :     :              +- *BroadcastHashJoin [tagId#627], [tagId#540], Inner, BuildRight\n",
      ":     :        :     :                 :- *Project [tagId#627, relevance#628]\n",
      ":     :        :     :                 :  +- *Filter ((((isnotnull(movieId#626) && (movieId#626 = 4306)) && isnotnull(relevance#628)) && (relevance#628 > 0.8)) && isnotnull(tagId#627))\n",
      ":     :        :     :                 :     +- *BatchedScan parquet [movieId#626,tagId#627,relevance#628] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/genome-scores, PartitionFilters: [], PushedFilters: [IsNotNull(movieId), EqualTo(movieId,4306), IsNotNull(relevance), GreaterThan(relevance,0.8), IsN..., ReadSchema: struct<movieId:int,tagId:int,relevance:double>\n",
      ":     :        :     :                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      ":     :        :     :                    +- *Project [tagId#540]\n",
      ":     :        :     :                       +- *Filter isnotnull(tagId#540)\n",
      ":     :        :     :                          +- *BatchedScan parquet [tagId#540] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/genome-tags, PartitionFilters: [], PushedFilters: [IsNotNull(tagId)], ReadSchema: struct<tagId:int>\n",
      ":     :        :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      ":     :        :        +- *Project [movieId#524, title#525]\n",
      ":     :        :           +- *Filter isnotnull(movieId#524)\n",
      ":     :        :              +- *BatchedScan parquet [movieId#524,title#525] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/movies, PartitionFilters: [], PushedFilters: [IsNotNull(movieId)], ReadSchema: struct<movieId:int,title:string>\n",
      ":     :        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      ":     :           +- *Project [tagId#629, tag#630]\n",
      ":     :              +- *Filter isnotnull(tagId#629)\n",
      ":     :                 +- *BatchedScan parquet [tagId#629,tag#630] Format: ParquetFormat, InputPaths: file:/D:/Sandbox/spark-intro/git/spark-intro/data/ml-latest/parquet/genome-tags, PartitionFilters: [], PushedFilters: [IsNotNull(tagId)], ReadSchema: struct<tagId:int,tag:string>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM relevant\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming\n",
    "\n",
    "The core of Spark Streaming is the DStream (discretized stream) analogous to RDD. \n",
    "Spark collects stream data in microbatches, creating multiple RDDs. Each batch, a new DStream is produced, containing all the RDDs collected during the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Real-time Word Popularity\n",
    "\n",
    "\n",
    "Let's start a netcat server. The server will listen on port 9999 and will send a line from the standard input each second (`-i 1` option).\n",
    "\n",
    "```bash\n",
    "$ nc -l -p 9999 -i 1 < data/book-allice-wonderland.txt\n",
    "```\n",
    "\n",
    "**wordcount-streaming.py**\n",
    "\n",
    "```python\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import re\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    hostname = \"localhost\"\n",
    "    port = 9999\n",
    "    \n",
    "    sc = SparkContext(appName=\"StreamingWordCountInPython\")\n",
    "    \n",
    "    # Necessary log4j logging level settings are done \n",
    "    log4j = sc._jvm.org.apache.log4j\n",
    "    log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)\n",
    "    \n",
    "    ssc = StreamingContext(sc, 2)\n",
    "    lines_ds = ssc.socketTextStream(hostname, port)\n",
    "    words = lines_ds.flatMap(lambda line: re.sub('\\W', ' ', line).split() )\n",
    "    counts_sorted = words.countByValue()    \\\n",
    "              .transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "    counts_sorted.pprint()\n",
    "\n",
    "    \n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "$ spark-submit --master=local[*] wordcount-streaming.py\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Processing\n",
    "\n",
    "- *Batch Interval* - The time interval at which a new DStream is produced. E.g. each 10 seconds\n",
    "- *Window Length* - The duration of batch intervals over which the operation is performed. E.g. messages last 5  minutes\n",
    "- *Slide Interval* - The interval at which the window operation is performed. E.g. every 20 seconds\n",
    "\n",
    "- *Block Interval*  - The duration of a micro-batch. Default is 200ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example - Online Word Popularity\n",
    "\n",
    "What if we want to monitor how the popularity of a word changes over time?\n",
    "We want to see what where the most popular words in the past 5 minutes.\n",
    "\n",
    "\n",
    "**wordcount-window.py**\n",
    "\n",
    "```python\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import re\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext(appName=\"PythonStreamingWordCount\")\n",
    "    \n",
    "    # Necessary log4j logging level settings are done \n",
    "    log4j = sc._jvm.org.apache.log4j\n",
    "    log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)\n",
    "    \n",
    "    # Create the Spark Streaming Context with 10 seconds batch interval\n",
    "    ssc = StreamingContext(sc, 10)\n",
    "    \n",
    "    # Set the check point directory for saving the data to recover when there is a crash\n",
    "    ssc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "    # Create a DStream that connects to localhost on port 9999\n",
    "    lines_ds = ssc.socketTextStream(\"localhost\", 9999)\n",
    "    words = lines_ds.flatMap(lambda line: re.sub('\\W', ' ', line).split() )\n",
    "    counts_sorted = words.countByValueAndWindow(300,10)   \\\n",
    "                   .transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "    counts_sorted.pprint()\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
